\documentclass{beamer}
\usepackage[utf8x]{inputenc}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{natbib}


\usetheme{Berlin}
\usecolortheme[RGB={0,150,150}]{structure}
\setbeamertemplate{navigation symbols}{}


\input{latex_macros_notation.tex}







\usepackage{amsthm}
\newtheorem{lem}{Lemma}
\newtheorem{dfn}{Definition}
\newtheorem{theo}{Theorem}





% \usepackage{beamerthemesplit} // Activate for custom appearance

\setbeamertemplate{footline}[frame number]
\AtBeginSection{\frame{\sectionpage}}



\title{Monte Carlo methods (and some Taylor) for Bayesian inference}
\author{Ingmar Schuster (FU Berlin)}
\date{\today}

\begin{document}

\frame{\titlepage}

\section[Outline]{}
\frame{\tableofcontents}

\section{Introduction}
\subsection{Honesty about uncertain model parameters}
\frame{
\frametitle{Point estimation vs. full posterior estimation (1)}
\begin{itemize}
	\item Point estimates of model parameters (e.g. from expectation maximization or other optimization strategies) can result in good statistical performance
	\item are very cheap for prediction and decision
	\item can use prior information (called regularization in optimization terms)
	\item are suboptimal
	\item using full posterior of a Bayesian model yields optimal performance for given prior (minimum expected error)
	
\end{itemize}
}

\frame{
	\frametitle{Point estimation vs. full posterior estimation (2)}
	\begin{itemize}
		\item assume there is a true parameter generating the data
		\begin{itemize}
			\item because of noise in data, typically won't find it using optimization
			\item as number of datapoints increases, posterior concentrates around true value
		\end{itemize}
		\item instead of giving a single point, we can estimate the region it lies in
		\begin{itemize}
			\item \emph{credible interval} region of posterior around mean which encloses certain percentage of posterior mass
			\item \emph{high posterior density region} every point in HPD region has posterior density no less than every point outside it, while covering certain percentage of posterior mass
			\item disconnected HPD region (multimodal posterior) can indicate misspecified model, i.e. prior and posterior don't match
		\end{itemize}
	\end{itemize}
}

\frame{
\frametitle{Credible interval}
\begin{center}
\includegraphics[width=\linewidth]{"fig/CI_multimodal_posterior"}
\end{center}
}

\frame{
	\frametitle{HPD region}
\begin{center}
\includegraphics[width=\linewidth]{"fig/HPD_multimodal_posterior"}
\end{center}
}

\subsection{Laplace approximation}
\frame{
\frametitle{Target approximation using 2nd order Taylor/Laplace (1)}
\begin{itemize}
	\item given some $\Rv^*$ s.t. $\frac{\partial \log \targd}{\Rv_i} = 0$ for all $i$ (local maximizer) and assume
	\begin{itemize}
		\item posterior density $\targd(\Rv) = p(\Rv|\dat)$ continuous in $\Rv$ 
		\item $\targd(\Rv) > 0$ for all $\Rv \in \Reals^\targdim$
		\item $\frac{\partial^2 \log \targd}{\partial \Rv_i \Rv_j}$ exists for all $i,j \in \{i,\targdim\}$
	\end{itemize}
	\item using matrix with entries $M_{i,j} = -\frac{\partial^2 \log \targd}{\partial \Rv_i \Rv_j}(\Rv^*)$ can construct second order Taylor approximation to $\log \targd$
	\item resulting in Gaussian approximation to $\targd$: $$\propd(\Rv) = \DNorm(\Rv;\Rv^*, M^{-1})$$
	\item $M^{-1}$ is covariance, $M$ precision matrix
	\item also called Laplace approximation
\end{itemize}
}

\frame{
\frametitle{Target approximation using 2nd order Taylor/Laplace (2)}
\begin{center}
\includegraphics[width=0.9\linewidth]{fig/Laplace_approximation_comic}
\end{center}

}

\frame{
	\frametitle{Target approximation using 2nd order Taylor/Laplace (3)}
	\textbf{Advantages}
	\begin{itemize}
		\item almost as fast as finding optimal point + $O(\targdim^3)$ for matrix inversion or cholesky decomposition
		\item rather accurate in high density regions
		\item Gaussian allows many follow-up calculation in closed form
	\end{itemize}
	\textbf{Disadvantages}
	\begin{itemize}
		\item doesn't work for distributions over discrete spaces
		\item bad representation of heavy tails and rare events
		\item asymptotic  correctness not attainable (no asymptotics at play)
	\end{itemize}
	
}


\section{Ordinary Monte Carlo and importance sampling}
\subsection{Ordinary Monte Carlo}
\frame{
\frametitle{Ordinary Monte Carlo}
\begin{itemize}
	\item Bayes optimal prediction for new input $\pred$ is expected $\outc$
	$$\outc_{opt} = \Expect_\targd [ \outc \, p(\outc|\pred,\Rv)] = \int \outc \, p(\outc|\pred,\Rv) \targd(\Rv) \mathrm{d} \Rv$$
	\item assume we can directly sample from distribution given by density $\targd(\Rv) = p(\Rv|\dat)$
	\item then  by the law of large numbers
	$$\outc_{opt} \approx \frac{1}{\nsamp} \sum_{i=1}^{\nsamp}  \sum_{\outc} \outc \,  p(\outc|\pred,\smp_i)$$ for finitely many values of $y$ and $\smp_i \sim \targd$ and
	 $$\outc_{opt} \approx \frac{1}{\nsamp} \sum_{i=1}^{\nsamp} \outcsmp_i$$
	for  $\outcsmp_i \sim p(\outc|\pred,\smp_i)$
\end{itemize}
}

\frame{
	\frametitle{Convergence rate of Monte Carlo}
	\begin{itemize}
		\item let $\outcest$ be the Monte Carlo (MC) estimate of $\outc_{opt}$
		\item MC is unbiased, i.e. $\Expect [\outcest] = \outc_{opt}$
		\item Standard deviation decreases at rate $O\left(\frac{1}{\sqrt{\nsamp}}\right)$
		\begin{align*}
		{\color{blue}\Var[\outcest]} &=\Expect\left[\left(\frac{1}{\nsamp} \sum_{i=1}^\nsamp \outcsmp_i - \Expect\left[\frac{1}{\nsamp} \sum_{i=1}^\nsamp \outcsmp_i \right]\right)^2\right] \\
		&= \frac{1}{\nsamp^2} \Var \left( \sum_{i=1}^\nsamp \outcsmp_i \right) 
		= {\color{blue}\frac{1}{\nsamp}  \Var (  \outcsmp_i )} \\
		\end{align*}
		\item unbiasedness and same rate hold for estimating $\Expect_\targd [\targfunc(\Rv)]$ for functional $\targfunc$ using Monte Carlo
	\end{itemize}
	\todo{Change to $\Rv$}
}

\frame{
\frametitle{Properties ordinary Monte Carlo}
\textbf{Advantages}
\begin{itemize}
	\item rate independent of dimension of $\Rv$, $\outc$
	\item unbiased
	\item can be applied when integral $\Expect_\targd [\targfunc(\Rv)]$ not given in closed form
\end{itemize}
\textbf{Disadvantages}
\begin{itemize}
	\item have to be able to sample from $\targd$
	\item if $\Var[\targfunc(\smp_i)]$ large, so is the variance of our estimator
\end{itemize}
}

\subsection{Importance sampling}

\frame{
\frametitle{Importance sampling trick}
\begin{itemize}
	\item if we can't sample from $\targd$, we can still use MC
	\begin{align*}
		{\color{blue}\targInt := \Expect_\targd [\targfunc(\Rv)]} &= \int \targfunc(\Rv) \targd(\Rv) \mathrm d \Rv =  \int \targfunc(\Rv) \frac{\targd(\Rv)}{\propd(\Rv)} \propd(\Rv) \mathrm d \Rv \\
		&\approx  {\color{blue} \frac{1}{\nsamp} \sum_{i=1}^{\nsamp}\targfunc(\smp_i)\iw_i =: \widehat{\targInt}_\propd}
	\end{align*}
	for $\smp_i \sim \propd$ and $\iw_i = {\targd(\smp_i)}/{\propd(\smp_i)}$
	\item called \emph{importance sampling} (IS)
	\item assumes that $\targd(\Rv) \targfunc(\Rv) \neq 0 \Rightarrow \propd(\Rv) > 0$
	\item assumes that $\targd$ can be evaluated in normalized form
\end{itemize}
}


\frame{
\frametitle{Importance sampling variance and confidence interval}
\begin{itemize}
	\item we have $\Var[\widehat{\targInt}_\propd] = \sigma_\propd^2/\nsamp$ and can estimate
	\begin{align*}
		\sigma_\propd^2 & = \Expect[(\targfunc(\smp_i)\iw_i)^2] - \Expect[\targfunc(\smp_i)\iw_i]^2 \\
		&= \Expect[(\targfunc(\smp_i)\iw_i - \Expect[\targfunc(\smp_i)\iw_i])^2] \\
		 &\approx \sum_{i=1}^{\nsamp}[(\targfunc(\smp_i)\iw_i - \widehat{\targInt}_\propd)^2]
	\end{align*} 
	\item confidence interval for estimator $\widehat{\targInt}_\propd$ using gaussian cdf method
\begin{center}
\includegraphics[width=0.7\linewidth]{fig/confidence_intervall}
\end{center}
%	\item when do we get high, when low variance?
\end{itemize}
}

\frame{
	\frametitle{Importance sampling variance minimization}
	\begin{itemize}
		\item how do we choose an optimal $\propd$?
		\item estimator always unbiased, so variance only source of error
		$$\sigma_\propd^2  = \Expect_\propd[(\targfunc(\smp_i)\iw_i)^2] - \Expect_\propd[\targfunc(\smp_i)\iw_i]^2 $$
		\item can be shown to be minimized at $$\propd(\Rv) = |\targfunc(\Rv)|\targd(\Rv)/\int|\targfunc(\Rv)|\targd(\Rv) \mathrm d \Rv$$
		\item so to get the optimal $\propd$ we need to solve the original problem (integration wrt $\targd$)!
		\item simple approach sample from Laplace approximation
		\item awful performance when proposal too narrow
	\end{itemize}
}

\frame{
\frametitle{Proposal density wider than target density}

\includegraphics{fig/IS_finite_variance_comic}

}

\frame{
	\frametitle{Target density wider than proposal density}
		\includegraphics{fig/IS_infinite_variance_comic}
}

\frame{
\frametitle{Importance sampling variance, non-normalized case}
\begin{itemize}
	\item as a rule of thumb, IS problematic when $$\lim_{|\Rv|\to \infty} \targd(\Rv)/\propd(\Rv) \to \infty$$
	\item a common strategy is to sample from heavy-tailed distributions (i.e. not Gaussians!)
	\item in Bayesian problems, we typically can't evaluate $\targd$ in normalized form, only $\tilde{\targd}(\Rv) = p(\dat|\Rv) p(\Rv)  =  \evid \, {\targd}(\Rv)$
	\item using standard IS with $\tilde \targd$ we approximate
	$$\int \targfunc(\Rv) \frac{\tilde{\targd}(\Rv)}{\propd(\Rv)} \propd(\Rv) \mathrm d \Rv = \evid \int \targfunc(\Rv) \frac{\targd(\Rv)}{\propd(\Rv)}\propd(\Rv) \mathrm d \Rv$$
	so how do we get rid of useless (for now) $\evid$?
	
\end{itemize}
}

\frame{
	\frametitle{Self-Normalized Importance Sampling}
	\begin{itemize}
		\item in case of unnormalized $\tilde \targd$ the \emph{self-normalized Importance Sampling} estimator can be used
		\item idea is to simply estimate $\evid$ using standard IS
		$$\int \tilde \targd(\Rv) \mathrm d \Rv = \int \evid \, \targd(\Rv) \mathrm d \Rv = \int \evid \frac{ \targd(\Rv)}{\propd(\Rv)} \mathrm d \Rv \approx \frac{1}{\nsamp} \sum_{i=1}^{\nsamp} \frac{ \targd(\smp_i)}{\propd(\smp_i)} = \widehat{\evid}$$
		for $\smp_i \sim \propd$
		\item when taking  $\iw_i = \frac{\targd(\Rv)}{\widehat{\evid}\propd(\Rv)}$ we again get $$\targInt \approx \sum_{i=1}^{\nsamp}[\targfunc(\smp_i)\iw_i] =  \widehat{\targInt}_\propd$$
		\item consistent by standard results, unbiased asymptotically in $\nsamp$, often has lower variance than standard IS \citep{Robert2004}
	\end{itemize}
}

\frame{
\frametitle{Effective sample size}
\begin{itemize}
	\item quality assessment: how many i.i.d. samples from $\targd$ are our samples from $\propd$ equivalent to
	\item one common measure is \emph{effective sample size}, estimated as
	$$ \nsamp_e = \frac{(\sum_{i=1}^{\nsamp} \iw_i)^2}{\sum_{i=1}^{\nsamp} \iw_i^2} $$
	where weights are normalized to sum to $1$ $$\iw_i = \frac{\targd(\smp_i)/\propd(\smp_i)}{\sum_{i=1}^{\nsamp} \targd(\smp_i)/\propd(\smp_i)}$$
\end{itemize}	
}

\subsection{Population Monte Carlo}

\frame {
	\frametitle{Adaptive Importance Sampling: Population Monte Carlo}
	\begin{itemize}
		\item Population Monte Carlo \citep[PMC;][]{Cappe2004}
		\begin{itemize}
			\item improves proposal distributions $\propd_{t}$ over iterations indexed by $t$ by adapting to samples from previous iterations
			\item resulting estimate is consistent by the law of large numbers 
			\begin{align*}
			\targInt =	\int \targfunc(\Rv) \targd(\Rv) \textrm{d}\Rv & = \int \targfunc(\Rv) \frac{\targd(\Rv)}{\propd(\Rv)} \propd(\Rv) \textrm{d}\Rv\\
			& = \iint \targfunc(\Rv) \frac{\targd(\Rv)}{\propd_t(\Rv)} \propd_t(\Rv) \textrm{d}\Rv~g(\propd_t)\textrm{d}\propd_t
			\end{align*}
			\item $g$ is any distribution on the $\propd_t$, so can adapt in any way we like
			\item simple special case of Sequential Monte Carlo
		\end{itemize}
	\end{itemize}
}

\frame {
	\frametitle{Population Monte Carlo \cite{Cappe2004}}
	\begin{algorithmic}
		\STATE {\bfseries Input:} initial proposal density $\propd_0$, unnormalized density $\tilde \targd $, population size $p$, sample size $m$
		\STATE {\bfseries Output:} lists $P,W$ of $m$ samples and weights
		
		
		\STATE Initialize $\smp = List()$
		\STATE Initialize $W = List()$
		\WHILE{$len(\smp) \leq \nsamp$}
		\STATE construct proposal distribution $\propd_{t}$
		\STATE generate  set of $p$ samples $\SaS_t$ from $\propd_{t}$ and append it to $\smp$
		\STATE ~~~for all $\smp \in \SaS_t$ append weights $\tilde\targd(\smp)/\propd_{t}(\smp)$ to $W$
		
		\ENDWHILE
	\end{algorithmic}
}

\frame{
	\frametitle{PMC properties}
	\begin{itemize}
		\item proposals $\propd_{t}$ must not degenerate to distributions that are `too thin` compared to $\targd $ 
		\item diminishing adaptation not a requirement, unlike in adaptive MCMC  (which we will see)
		\item can adapt any which way we like without need to proof ergodicity
		\item probability of model given the data at any time for model choice and Bayesian testing (this is $\evid$!)
		\item using randomized Low Discrepancy point sets, we can improve convergence rates more easily than in Metropolis-Hastings
	\end{itemize}
}

\frame{
	\frametitle{Random-Walk PMC (1)}
	\begin{itemize}
		\item pick some $\propd_0$, for example some Gaussian or Student-$t$ distribution
		\item given samples $\smp_{t-1,i}$ from iteration $t-1$, choose as next proposal the mixture of Gaussians
		$$\propd_t = \sum_{i=1}^{p}\DNorm(\smp_{t-1,i}, M)$$
		for fixed covariance matrix M
		\item iterate for desired number of iterations
	\end{itemize}
}

\frame{
	\frametitle{Random-Walk PMC (2)}
	
\includegraphics[width=\linewidth]{fig/PMC_rw_comic}

}

\frame{
	\frametitle{PMC properties}
	\textbf{Advantages}
	\begin{itemize}		
		\item simple to understand
		\item often performs better empirically than Metropolis-Hastings with same proposal distribution (next lecture)
	\end{itemize}
	\textbf{Disadvantages}
	\begin{itemize}		
		\item Central Limit Theorems only for special cases
		\item not as widely used as Metropolis-Hastings
		\item not  as well understood as Metropolis-Hastings and MCMC in general
	\end{itemize}
}
\subsection*{}
\frame{
	\frametitle{Conclusion ordinary MC and IS}
	\begin{itemize}
		\item ordinary Monte Carlo and convergence rate $O(1/\sqrt{\nsamp})$
		\item importance sampling
		\begin{itemize}
			\item standard case: use MC when we cant sample from target
			\item self-normalized case: when target density is not normalized
			\item adaptive schemes: Population Monte Carlo
		\end{itemize}
	\end{itemize}
}

\section{Markov Chain Monte Carlo}


\frame{
\frametitle{Sampling by constructing a Markov process}
\begin{itemize}
	\item we might sample from arbitrary density $\targd$ by constructing Markov process that has $\targd$ as 'invariant' distribution
	\item i.e. start with some point $\smp_0$, generate point $\smp_1$ from constructed Markov process
	\item if process has invariant distribution $\targd$, then 
	$$\smp_0 \sim \targd \Rightarrow \smp_1 \sim \targd$$
	\item some constructions guarantee $\smp_t \sim \targd$ for some $t\in \Nats$, no matter what the distribution of $\smp_0$ is
	\item these methods are called \emph{Markov Chain Monte Carlo} (MCMC)
\end{itemize}
}

\frame{
	\frametitle{Detailed balance}
	\begin{itemize}
		\item denote transition probability from current state $\Rv$ to new state $\Rv'$ by $\MK(\Rv'|\Rv)$
		$$\Rv \rightarrow_{\MK(\cdot|\Rv)} \Rv'$$
		\item one way of constructing a Markov chain leaving some $\targd$ invariant is to satisfy detailed balance wrt $\targd$
		$$ \targd(\Rv)\MK(\Rv'|\Rv) = \targd(\Rv')\MK(\Rv|\Rv') $$
		\emph{Proof of invariance (sketch)}:  
		\begin{align*}
		\int \targd(\Rv)\MK(\Rv'|\Rv) \mathrm d \Rv & = \int \targd(\Rv')\MK(\Rv|\Rv') \mathrm d \Rv  \\
		 & = \targd(\Rv')  \underbrace{\int \MK(\Rv|\Rv') \mathrm d \Rv}_{=1} \\
		 & = \targd(\Rv')  &
		\end{align*}
	\end{itemize}
}

\frame{
	\frametitle{Metropolis-Hastings construction}
	\begin{itemize}
		\item idea: construct algorithm that satisfies detailed balance
		\item use two parts
		\begin{itemize}
			\item a distribution for proposing a move $\propd(\cdot|\Rv)$
			\item probability for accepting proposed move $\acc(\cdot|\Rv)$ (else stay at current state)
		\end{itemize}
		\item Algorithm
		\begin{itemize}
			\item at state $\Rv$, draw sample $\Rv' \sim \propd(\cdot|\Rv)$
			\item with probability $\acc(\Rv'|\Rv)$, new chain state is $\Rv'$, else $\Rv$
		\end{itemize}
	\end{itemize}
}

\frame{
	\frametitle{MH Markov kernels}
	\begin{itemize}
		\item at state $\Rv$, draw sample $\Rv' \sim \propd(\cdot|\Rv)$
		\item with probability $\acc(\Rv'|\Rv)$, new chain state is $\Rv'$, else $\Rv$
	
	\begin{align*}
	 \MK(\Rv'|\Rv) & = \propd(\Rv'|\Rv)\acc(\Rv'|\Rv) \\
	 \MK(\Rv|\Rv)  & = \propd(\Rv|\Rv)+\int \propd(\Rv'|\Rv)(1-\acc(\Rv'|\Rv)) \mathrm d \Rv'
	\end{align*}
		\item if $\propd$ is fixed we need to find $\acc$ such that detailed balance holds
	\end{itemize}
}

\frame{
	\frametitle{MH acceptance probability}
	\begin{itemize}
		\item Algorithm
		\begin{itemize}
			\item at state $\Rv$, draw sample $\Rv' \sim \propd(\cdot|\Rv)$
			\item with probability $\acc(\Rv'|\Rv)$, new chain state is $\Rv'$, else $\Rv$
		\end{itemize}
		\item from detailed balance for $\Rv \neq \Rv'$
		\begin{align*}
		 \targd(\Rv)\MK(\Rv'|\Rv) &= \targd(\Rv')\MK(\Rv|\Rv') & \Leftrightarrow\\
		\targd(\Rv)\propd(\Rv'|\Rv)\acc(\Rv'|\Rv) &= \targd(\Rv')\propd(\Rv|\Rv')\acc(\Rv|\Rv') & \Leftrightarrow\\
		\frac{\acc(\Rv'|\Rv)}{\acc(\Rv|\Rv')} &= \frac{\targd(\Rv')\propd(\Rv|\Rv')}{\targd(\Rv)\propd(\Rv'|\Rv)} &
		\end{align*}
		\item this equality holds for the choice
		$$  \acc(\Rv'|\Rv) = \min\left(1, \frac{\targd(\Rv')\propd(\Rv|\Rv')}{\targd(\Rv)\propd(\Rv'|\Rv)}\right) $$
	\end{itemize}
}

\frame{
\frametitle{MH acceptance probability choice and detailed balance}
Pick
$ \acc(\Rv'|\Rv) = \min\left(1, r \right) $
for
$ r := \frac{\targd(\Rv')\propd(\Rv|\Rv')}{\targd(\Rv)\propd(\Rv'|\Rv)}$.\\
Then detailed balance holds if
$$\frac{\acc(\Rv'|\Rv)}{\acc(\Rv|\Rv')} = r$$
case $r < 1$:
$$\frac{\acc(\Rv'|\Rv)}{\acc(\Rv|\Rv')} = \frac{\min\left(1, r \right)}{\min\left(1, 1/r \right)} = \frac{r}{1} = r$$
case $r \geq 1$:
$$\frac{\acc(\Rv'|\Rv)}{\acc(\Rv|\Rv')} = \frac{\min\left(1, r \right)}{\min\left(1, 1/r \right)} = \frac{1}{1/r} = r$$
}

\frame{
\frametitle{Average acceptance probabilities}
\begin{figure}
\centering
\includegraphics[width=\linewidth]{fig/Acceptance_rates_high_to_low}
\caption{Average acceptance close to $1$ (left, highly correlated) close to $0.5$ (middle) and close to $0$ (right, highly correlated)}
\label{fig:Acceptance_rates_high_to_low}
\end{figure}

}


\frame{
	\frametitle{Independent MH and random walk MH}
	
	\begin{tabular}{cc}
		\textbf{Independent MH} & \textbf{random walk MH}\\
		\includegraphics[width=0.45\linewidth]{fig/MH_indep_comic}&
		\includegraphics[width=0.45\linewidth]{fig/MH_rw_comic}\\
		time-independent & centered at current state
	\end{tabular}
}

\frame{
	\frametitle{Random walk MH}
	\begin{itemize}
		\item most common choice is random walk (RW) proposal
		\item often times performs better, as it adapts to target
		\item joint probability of current state and proposal given by
		$$ \targd(\Rv) \propd(\Rv'|\Rv) $$
		\item marginal proposal $\propd_{mrg}(\Rv') = \Expect_\targd [\propd(\Rv'|\Rv)] = \int \targd(\Rv) \propd(\Rv'|\Rv) \mathrm d \Rv$
		\item when proposal is e.g. Gaussian, $\propd_{mrg}$ is target convoluted with Gaussian kernel
	\end{itemize}
}

\frame{
\frametitle{Marginal proposal for RW-MH}
	
\begin{figure}
\centering
\includegraphics[width=\linewidth]{fig/MH_marg_comic}
\caption{Marginal proposal density for Gaussian RW-MH}
\label{fig:MH_marg_comic}
\end{figure}

}

\frame{
	\frametitle{Choice of proposal (1)}
	\begin{itemize}
		\item must ensure reversibility
		$$ \propd(\Rv'|\Rv) > 0\Rightarrow \propd(\Rv|\Rv') > 0$$
		\item opposing goals
		\begin{itemize}
			\item $\propd$ should be broad to quickly cover all the space
			\item but then many proposed points will be rejected because of low value under $\targd$
		\end{itemize}
		
	\end{itemize}
	
\begin{center}
	\includegraphics[width=0.7\linewidth]{fig/MH_rw_width_comic}\\
	\small $\acc(\Rv'|\Rv) = \min\left(1, {\targd(\Rv')\propd(\Rv|\Rv')}/{\targd(\Rv)\propd(\Rv'|\Rv)}\right)$
\end{center}
	}


\frame{
	\frametitle{Choice of proposal (2)}
	\begin{itemize}
		\item if all moves are accepted, we probably propose very small steps
		\item if all moves are rejected, steps are very large (proposal variance high)
		\item intuitively, average acceptance probability shouldn't be close to $0$ or to $1$, rather in between
		\item under strong conditions on target $\targd$, optimal acceptace rates for certain $\propd$ have been derived
		\begin{itemize}
			\item $\propd(\cdot|\Rv) = \DNorm(\cdot; \Rv, \param M)$: $\acc_{opt} \approx 0.234$ for dimensionality $>5$
			\item $\propd(\cdot|\Rv) = \DNorm(\cdot; \Rv + \lambda \nabla \log \targd(\Rv), \param M)$: $\acc_{opt} \approx 0.56$
			\item \dots
		\end{itemize}
		\item ideal: find parameters $\lambda, M, \param$ s.t. we attain optimal average acceptance rate
	\end{itemize}
}

\frame{
	\frametitle{Optimal proposal (1)}
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{fig/Acceptance_rates_high_low_optimal_iid}
\caption{\emph{Top}: Too high high (left, $0.96$) and too low (right, $0.13$) acceptance rates.
	\emph{Bottom}: approx. optimal rate for 1D target (left, $0.47$), i.i.d. target samples (right).
	}
\label{fig:Acceptance_rates_high_low_optimal_iid}
\end{figure}

}

\frame{
	\frametitle{Optimal proposal (2)}
	\begin{itemize}
		\item assume we have a single scaling parameter as in $\propd(\cdot|\Rv) = \DNorm(\cdot; \Rv, \param M)$
		\item increasing $\param$ decreases average acceptance rate and vice versa
		\item naive tuning
		\begin{itemize}
			\item run chains at different values for $\param$
			\item compute average acceptance rate
			\item iterate until good value found
		\end{itemize}
		\item all asymptotic arguments from Markov chain theory still hold
		\item computationally costly
	\end{itemize}
}

\frame{
	\frametitle{Optimal proposal (3)}
	\begin{itemize}
		\item On-line algorithm for tuning $\param$: set
		$$ \parsmp_{i+1} = \parsmp_i + \ssize_i (\hat\alpha_{\parsmp_i} - \alpha_{opt})$$
		for sequence of step sizes $\lim_{i \to \infty}\ssize_i = 0$ and acceptance rate estimate $\hat\alpha_{\parsmp_i}$
		\item With an explicit noise term $(\hat\alpha_{\param_i} - \bar\alpha_{\param_i})$ this can be written as
		\begin{equation*}
		\label{eq:stoch_appr}
		\parsmp_{i+1} = \parsmp_i + \ssize_i ( (\bar\alpha_{\parsmp_i} - \alpha_{opt})+ (\hat\alpha_{\parsmp_i} - \bar\alpha_{\parsmp_i}))
		\end{equation*}
		where $\bar\alpha_{\parsmp_i}$ is actual acceptance rate for $\param = \parsmp_i$
		\item under mild conditions sequence of noise terms $(\hat\alpha_{\parsmp_i} - \bar\alpha_{\parsmp_i})_{i=1}^\infty$ averages to zero
	\end{itemize}
	
	}

\frame{
	\frametitle{Adaptive MH algorithms}
	\begin{itemize}
		\item this is an example of an adaptive MH algorithm
		\item while algorithm class is often called adaptive Markov chain MC, it is not Markovian any more
		\item proposal density depends on all previous samples through $\parsmp_i$, not just on current state
		\item guarantee that we sample from $\targd$ asymptotically crucially depends on condition  $\lim_{i \to \infty}\ssize_i = 0$
		\item as $\ssize_i \rightarrow 0$, we use Markovian MH algorithm with fixed $\param$
		\item example of \emph{diminishing adaptation condition} for adaptive MH
	\end{itemize}
	
}

\frame{
	\frametitle{Adaptive MCMC algorithms}
	To asymptotically sample from $\targd$, adaptive MCMC must satisfy  
	\begin{itemize}
		\item \emph{Diminishing Adaptation}
		\begin{equation*}
		\label{eq:dimin_adapt}
		\lim_{n \rightarrow \infty} \sup_{\Rv \in \StateSp} \|\MK_{n+1}(\Rv, \cdot) - \MK_{n}(\Rv, \cdot)\| = 0 \textrm{ in probability}
		\end{equation*}
		\item \emph{Bounded Convergence} (or \emph{Containment})
		\begin{equation*}
		\label{eq:bound_conv}
		\{M_\epsilon(\smp_n, n)\}_{n=0}^\infty \textrm{ is bounded in probability}
		\end{equation*} 
		where $M_\epsilon(\Rv, n) = \inf\{\|\MK_n^i(\Rv, \cdot) - \targd(\cdot) \| \leq \epsilon : i \geq 1 \}$ for some $\epsilon > 0$ is called the $\epsilon$ convergence time for kernel $\MK_n$ when starting from state $\Rv$. 
	\end{itemize}
}

\frame{
	\frametitle{Diminishing adaptation}
	\begin{itemize}
		\item diminishing adaptation is stronger conditions
		\begin{equation*}
		\label{eq:dimin_adapt}
		\lim_{n \rightarrow \infty} \sup_{\Rv \in \StateSp} \|\MK_{n+1}(\Rv, \cdot) - \MK_{n}(\Rv, \cdot)\| = 0 \textrm{ in probability}
		\end{equation*}
		\item amount of adaptation goes to $0$ for a growing number of samples $n$
		\item infinite overall adaptation still possible $\sum_{n=1}^\infty \sup_{x \in \StateSp} \|\MK_{n+1}(x, \cdot) - \MK_{n}(x, \cdot)\|< \infty$
		\item limit $ \lim_{n \rightarrow \infty} \MK_n$ does not need to exist (no convergence to some fixed kernel necessary)
		\item one way of diminishing adaptation: adapt with probability $p(n)$ and have $\lim_{n \rightarrow \infty} p(n) = 0$. 
	\end{itemize}
}

\frame{
\frametitle{Bounded convergence}
\begin{itemize}
	\item {Bounded Convergence} is less strong
	\begin{equation*}
	\label{eq:bound_conv}
	\{M_\epsilon(\smp_n, n)\}_{n=0}^\infty \textrm{ is bounded in probability}
	\end{equation*}
	with  $M_\epsilon(\Rv, n) = \inf\{\|\MK_n^i(\Rv, \cdot) - \targd(\cdot) \| \leq \epsilon : i \geq 1 \}$ for some $\epsilon > 0$
	\item intuition: no matter what Markov kernel  $\MK_n$ and current state $\smp_n$,
	iterating $\MK_n$ will converge in a finite number of iterations
	\item while typically easy to satisfy, counterexamples exist
\end{itemize}
}

\frame{
\frametitle{Adaptive Metropolis (1)}
\begin{itemize}
	\item one of the first adaptive algorithms is \emph{adaptive Metropolis}
	\item basic idea is to adapt to covariance structure of target
	\item using samples resulting from accept/reject step	
	\item under assumption that target variances are finite (i.e. covariance matrix exists)
	\item amounts to finding directions and amount of main variation
\end{itemize}
}

\frame{
	\frametitle{Adaptive Metropolis (2)}

	\begin{itemize}
		\item concretely, we use covariance
			\begin{equation*}
			\label{eq:haario_cov}
			C_n = \begin{cases}
			C_0 & n \leq n_0 \\
			\param (\Cov(X_0,\dots, X_{n-1}) + \epsilon I)& n > n_0
			\end{cases}
			\end{equation*}
			at iteration $n$, where $\param$ and $\epsilon$ are tunable parameters
		\item  update $C_n$ recursively, or directly a Cholesky decomposition to avoid $O(\targdim)$ cost at each iteration
		\item proposal distribution for a new point
		\begin{equation*}
		q_\mathrm{AM}(\cdot|\smp_n) = \DNorm(\cdot; \smp_n, C_n)
		\end{equation*}
		\item standard acceptance probability
	\end{itemize}
}

\frame{
	\frametitle{Adaptive Metropolis (3)}
	\begin{itemize}
		\item computationally relatively cheap
		\item diminishing adaptation at $O(1/n)$ rate (covariance estimate is a MC average)
		\item bounded convergence if density $\targd$ decays at least polynomially
		\begin{itemize}
			\item ensures entries in $C_n$ converge to finite value
		\end{itemize}
		\item can be combined with on-line tuning of $\param$ to achieve optimal RW acceptance rate of $0.234$
		\item strong performance for many problems
	\end{itemize}
}

\subsection*{Conclusion MCMC}
\frame{
\frametitle{Conclusion MCMC (1)}
\begin{itemize}
	\item have looked at general idea of constructing a Markov chain to sample from a desired target
	\item introduced most important class of MCMC, Metropolis-Hastings
	\begin{itemize}
		\item construction using proposal density and acceptance probability
		\item construction of acceptance probability to satisfy detailed balance
		\item time-independent proposals vs. time-dependent proposals (e.g. random walk proposals)
	\end{itemize}
\end{itemize}
}

\frame{
\frametitle{Conclusion MCMC (2)}
 \begin{itemize}
	\item other MCMC algorithms exist to achieve same goal (Gibbs sampling, unadjusted Langevin algorithm)
	\item adaptive MH algorithms and conditions for convergence
	\begin{itemize}
		\item diminishing adaptation and bounded convergence 
		\item on-line tuning of scaling parameter
		\item Adaptive Metropolis
	\end{itemize}
\end{itemize}
}


\frame{
\centerline{\LARGE \emph{Thanks!}}
}

\AtBeginSection{}
\frame[allowframebreaks]{
\frametitle{Literature}
\bibliographystyle{apalike}
\bibliography{library}
}
\end{document}
